{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD,RMSprop, Adam\n",
    "from keras.utils import np_utils\n",
    "from keras import metrics\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "992 train samples\n",
      "Type_2    530\n",
      "Type_3    286\n",
      "Type_1    176\n",
      "dtype: int64\n",
      "489 test samples\n",
      "Type_2    251\n",
      "Type_3    164\n",
      "Type_1     74\n",
      "dtype: int64\n",
      "512 Submission samples\n"
     ]
    }
   ],
   "source": [
    "### Get the data \n",
    "#training\n",
    "train = read_csv(\"./features_2iter/train_v3.csv\")\n",
    "array_train = train.values\n",
    "X= array_train[:,3:200]\n",
    "y = array_train[:,0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33, random_state=42)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(pd.value_counts(pd.Series(y_train)))\n",
    "\n",
    "print(X_test.shape[0], 'test samples')\n",
    "print(pd.value_counts(pd.Series(y_test)))\n",
    "\n",
    "\n",
    "# Generate dummy target \n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "encoded_Y = encoder.transform(y_train)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "y_train_dummy = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "encoder.fit(y_test)\n",
    "encoded_Y = encoder.transform(y_test)\n",
    "y_test_dummy = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "\n",
    "### Get the submission data\n",
    "test = read_csv(\"./features_2iter/test_v3.csv\")\n",
    "array_test = test.values\n",
    "X_sub= array_test[:,2:200]\n",
    "print(X_sub.shape[0], 'Submission samples')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set parameters\n",
    "NB_CLASSES=3\n",
    "N_HIDDEN= X_train.shape[1]\n",
    "DROPOUT=0.5\n",
    "#OPTIMIZER=SGD(lr=1e-2, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#OPTIMIZER=Adam()\n",
    "\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(activation='relu',  dropout_rate=0.0, optimizer='rmsprop', init_mode='glorot_uniform'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_shape=(N_HIDDEN,) ,kernel_initializer=init_mode))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(neurons),kernel_initializer=init_mode)\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(NB_CLASSES),kernel_initializer=init_mode)\n",
    "    model.add(Activation('softmax'))\n",
    "    #model.summary()\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "# grid search epochs, batch size and optimizer\n",
    "optimizers = ['rmsprop', 'adam','adadelta']\n",
    "init = ['glorot_uniform', 'normal', 'uniform']\n",
    "activation = ['softplus', 'relu', 'tanh', 'linear']\n",
    "dropout_rate = [0.0, 0.5, 0.8]\n",
    "neurons=[50,100,200]\n",
    "epochs = [20,30, 50]\n",
    "batches = [10, 50,100]\n",
    "\n",
    "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, \n",
    "                  init_mode=init_mode, activation=activation,dropout_rate=dropout_rate,neurons=neurons)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train_dummy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
